x-base_service: &base_service
    ports:
      - "${WEBUI_PORT:-7860}:7860"
    volumes:
      - &v1 ./data:/data
      - &v2 ./output:/output
      
     
    stop_signal: SIGKILL
    tty: true
    deploy:
      resources:
        reservations:
          devices:
              - driver: nvidia
                device_ids: ['0']
                capabilities: [compute, utility]

name: cody-webui-docker

volumes:
  ollama_data:
  openwebui_data:
  ollama-modelsv1:
    name: "ollama-modelsv1"
    external: true

services:
  download:
    build: ./services/download/
    profiles: ["download"]
    volumes:
      - *v1

  auto: &automatic
    <<: *base_service
    profiles: ["auto", "ai"]
    build: ./services/AUTOMATIC1111
    image: sd-auto:78
    environment:
      - CLI_ARGS=--allow-code --medvram --xformers --enable-insecure-extension-access --api


  # auto-cpu:
    # <<: *automatic
    # profiles: ["auto-cpu", "ai"]
    # deploy: {}
    # environment:
      # - CLI_ARGS=--no-half --precision full --allow-code --enable-insecure-extension-access --api

  # comfy: &comfy
    # <<: *base_service
    # profiles: ["comfy", "ai"]
    # build: ./services/comfy/
    # image: sd-comfy:7
    # environment:
      # - CLI_ARGS=


  # comfy-cpu:
    # <<: *comfy
    # profiles: ["comfy-cpu", "ai"]
    # deploy: {}
    # environment:
      # - CLI_ARGS=--cpu

  # ollama provides the framework for  runnning the LLM's
  ollama:
    <<: *base_service
    image: ollama/ollama:latest  
    container_name: ollama
    profiles: ["ai"]
    ports:
      - "11434:11434"
    volumes:
      # persist models in ./models on host machine
      # translates to the llocation of the docker-compose.yaml on host machine
      #- "./models:/root/.ollama/models/blobs"
      - "ollama-modelsv1:/root/.ollama/models/blobs"
      
    deploy:
        resources:
            reservations:
                devices:
                  - driver: nvidia
                    count: 1
                    capabilities: [gpu]
  # openwebui provides the UI as well as the API, RAG and other advanced configurations for the LLM's
  openwebui:
    image: ghcr.io/open-webui/open-webui:latest  
    container_name: openwebui
    profiles: ["ai"]
    ports:
      - "5555:8080"
    environment:
      # EnvVars from: https://docs.openwebui.com/getting-started/advanced-topics/env-configuration/#image-generation
      - ENABLE_IMAGE_GENERATION=true
      - IMAGE_GENERATION_ENGINE=automatic1111
      - AUTOMATIC1111_BASE_URL=http://auto:7860 # the internal adress of the auto  container
      - CONTENT_EXTRACTION_ENGINE=tika
      - TIKA_SERVER_URL=http://tika:9998
  tika:
    image: apache/tika:latest-full # make sure to take full, the other ones do not have tesseract ocr!
    container_name: tika
    profiles: ["ai"]
    ports:
      - 9998:9998
  openedaispeech:
    build:
      dockerfile: Dockerfile
    image: ghcr.io/matatonic/openedai-speech
    env_file: speech.env
    profiles: ["ai"]
    ports:
      - "8000:8000"
    volumes:
      - ./voices:/app/voices
      - ./config:/app/config
    # To install as a service
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              #device_ids: ['0', '1'] # Select a gpu, or
              count: all
              capabilities: [gpu]
  nginx:
    image: nginx:latest
    container_name: nginx
    profiles: ["ai"]
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
      - ./certs:/etc/nginx/certs
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - openwebui

  unslothtrainer:
    build: ./services/unslothtrainer
    image: unslothtrainer
    container_name: unslothtrainer
    profiles: ["ai"]
    volumes:
      # persist models in ./models on host machine
      # translates to the llocation of the docker-compose.yaml on host machine
      #- "./models:/root/.ollama/models/blobs"
      - "ollama-modelsv1:/root/.ollama/models/blobs"
    ports:
      - "8888:8888"